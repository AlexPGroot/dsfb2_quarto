[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "testing_qmd",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n2 + 2\n\n[1] 4",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "005_open_peer_review.html",
    "href": "005_open_peer_review.html",
    "title": "Open peer review",
    "section": "",
    "text": "Open peer review\nWrite an Rmarkdown report on your findings, including the table above and some information about the article such as general aim, short methods and results. If data is available, try including some\nThis exercise is about identifying reproducibility issues in a scientific publication. The reproducibility is scored by the metrics used here.\nPublications do not always contain a ‘data availability statement’, and those that do do not always contain any data. Statements range from ‘not publicly available’, ‘available on reasonable request’, ‘publicly available’ or ‘publicly available, except not here and there is no link or text supplying said data’.\nWe looked at this paper in this exercise:\nJason C.K. Chan, Krista D. Manley, Dahwi Ahn, Does retrieval potentiate new learning when retrieval stops but new learning continues?, Journal of Memory and Language, Volume 115, 2020, 104150, ISSN 0749-596X https://doi.org/10.1016/j.jml.2020.104150\nThe general aim of the experiment described in the article has to do with a concept called “the forward testing effect”. This is a mechanic that enhances a learner’s ability to learn new materials, which is a result of “interpolated retrieval opportunities” such as brief quizzes between learning sessions. The article examines the persistence of the forward testing effect when the students stop receiving these interpolated retrieval opportunities.\nThis was tested through four different experiments. The general experimental setup was to have undergrad students from Iowa State University participate instructed to study a list of words with the goal of having a test at the end. The students were given learning instructions and were informed that they might randomly be given interpolated tests by the computer. The experiments differed in at which moments the interpolated tests occurred and the experimental results were measured by the student’s final performance.\nThe study suggests that consistently performing these retrieval opportunities provides a benefit to retaining the information as observed by the improved results. When the interpolated testing is stopped however, its advantages are diminished substantially.\nUsing the metrics from the aforementioned initial publication we score this paper as follows:\n\n\n\n\n\n\n\n\nTransparency Criteria\nDefinition\nResponse\n\n\n\n\nStudy Purpose\nA concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.\nYes\n\n\nData Availability Statement\nA statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.\nNo, supplementary only\n\n\nData Location\nWhere the article’s data can be accessed, either raw or processed.\nNo\n\n\nStudy Location\nAuthor has stated in the methods section where the study took place or the data’s country/region of origin.\nNo, participant’s origin only\n\n\nAuthor Review\nThe professionalism of the contact information that the author has provided in the manuscript.\nYes, email\n\n\nEthics Statement\nA statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.\nNo\n\n\nFunding Statement\nA statement within the manuscript indicating whether or not the authors received funding for their research.\nNo\n\n\nCode Availability\nAuthors have shared access to the most updated code that they used in their study, including code used for analysis.\nNo\n\n\n\nWhile the paper states that it has supplementary data available with a doi linking to the paper: https://doi.org/10.1016/j.jml.2020.104150 It does not contain an actual data availability statement as mentioned before, however upon further inspection the results under experiment 1 does contain a broken link https://doi.org//10.17605/OSF.IO/ G2Y93 due to a space and a working url https://osf.io/g2y93 to the OSF version which contains the data used in the paper.\n\nfs::dir_tree(here::here(\"data_raw/data_0050/osfstorage-archive/\"))\n\n/home/alexgroot/dsfb2_quarto_qmd/data_raw/data_0050/osfstorage-archive/\n├── TMNT Combined Data OSF.csv\n├── TMNT Meta-analysis OSF.csv\n├── TPL-TMNT E1 Data OSF.csv\n├── TPL-TMNT E2 Data OSF.csv\n├── TPL-TMNT E3 Data OSF.csv\n├── TPL-TMNT E4 Data OSF.csv\n└── readme.txt\n\nlibrary(tidyverse) # REMOVE\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nFiles are clearly named and a readme.txt containing metadata file is included.\n\nread.csv(here::here(\"data_raw/data_0050/osfstorage-archive/TPL-TMNT E1 Data OSF.csv\")) %&gt;% head()\n\n      Condition Subject Session L1_RclP L2_RclP L3_RclP L4_RclP Int1_4 Int2_4\n1 Always-Tested     300       1    1.00    1.00    1.00    0.93      0      0\n2 Always-Tested     301       2    1.00    1.00    0.93    1.00      0      0\n3 Always-Tested     302       3    0.87    0.87    0.93    0.73      0      0\n4 Always-Tested     303       4    0.73    0.67    0.87    0.73      0      0\n5 Always-Tested     304       1    0.93    0.47    0.80    1.00      0      0\n6 Always-Tested     305       2    0.60    0.73    0.73    0.87      0      0\n  Int3_4 Int_L4 L1_ARC L2_ARC L3_ARC L4_ARC\n1      0      0   1.00   1.00   0.86   1.00\n2      0      0   0.30   0.50   0.84   1.00\n3      0      0   0.05   0.37   1.00   1.00\n4      0      0   0.74   0.45   0.84   0.78\n5      0      0   0.58   0.61   1.00   0.88\n6      0      0  -0.04   0.34   0.47   1.00\n\n\nThe data provided is in tidy format, which makes for easy data manipulation and analysis. The data does not contain any R code, however the analysis in the original paper was performed in Jeffreys’s Amazing Statistics Program (JASP), which is a program written in C++ and QML, but the analyses themselves are written in R using packages from CRAN.\n\n# load data\ndata &lt;- read.csv(here::here(\"data_raw/data_0050/osfstorage-archive/TPL-TMNT E1 Data OSF.csv\"))\n\n# check the data\ndata %&gt;% \n  summary()\n\n  Condition            Subject         Session        L1_RclP      \n Length:120         Min.   :101.0   Min.   :1.00   Min.   :0.0000  \n Class :character   1st Qu.:139.5   1st Qu.:1.75   1st Qu.:0.5300  \n Mode  :character   Median :233.5   Median :2.50   Median :0.7300  \n                    Mean   :227.5   Mean   :2.50   Mean   :0.6784  \n                    3rd Qu.:311.2   3rd Qu.:3.25   3rd Qu.:0.8700  \n                    Max.   :353.0   Max.   :4.00   Max.   :1.0000  \n                                                   NA's   :40      \n    L2_RclP          L3_RclP          L4_RclP           Int1_4      \n Min.   :0.0700   Min.   :0.2700   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5825   1st Qu.:0.5825   1st Qu.:0.3300   1st Qu.:0.0000  \n Median :0.7650   Median :0.8000   Median :0.5300   Median :0.0000  \n Mean   :0.7065   Mean   :0.7342   Mean   :0.5281   Mean   :0.1833  \n 3rd Qu.:0.8850   3rd Qu.:0.9300   3rd Qu.:0.7300   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :4.0000  \n NA's   :80       NA's   :80                                        \n     Int2_4          Int3_4           Int_L4        L1_ARC       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0   Min.   :-1.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.: 0.0650  \n Median :0.000   Median :0.0000   Median :1.0   Median : 0.3600  \n Mean   :0.425   Mean   :0.4917   Mean   :1.1   Mean   : 0.3470  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0   3rd Qu.: 0.7525  \n Max.   :8.000   Max.   :4.0000   Max.   :9.0   Max.   : 1.0000  \n                                                NA's   :40       \n     L2_ARC           L3_ARC            L4_ARC       \n Min.   :-0.670   Min.   :-1.0000   Min.   :-3.0000  \n 1st Qu.: 0.330   1st Qu.: 0.4550   1st Qu.: 0.0000  \n Median : 0.650   Median : 0.8400   Median : 0.4550  \n Mean   : 0.525   Mean   : 0.6368   Mean   : 0.3427  \n 3rd Qu.: 0.865   3rd Qu.: 1.0000   3rd Qu.: 1.0000  \n Max.   : 1.000   Max.   : 1.0000   Max.   : 1.0000  \n NA's   :80       NA's   :80                         \n\ndata %&gt;%\n  head()\n\n      Condition Subject Session L1_RclP L2_RclP L3_RclP L4_RclP Int1_4 Int2_4\n1 Always-Tested     300       1    1.00    1.00    1.00    0.93      0      0\n2 Always-Tested     301       2    1.00    1.00    0.93    1.00      0      0\n3 Always-Tested     302       3    0.87    0.87    0.93    0.73      0      0\n4 Always-Tested     303       4    0.73    0.67    0.87    0.73      0      0\n5 Always-Tested     304       1    0.93    0.47    0.80    1.00      0      0\n6 Always-Tested     305       2    0.60    0.73    0.73    0.87      0      0\n  Int3_4 Int_L4 L1_ARC L2_ARC L3_ARC L4_ARC\n1      0      0   1.00   1.00   0.86   1.00\n2      0      0   0.30   0.50   0.84   1.00\n3      0      0   0.05   0.37   1.00   1.00\n4      0      0   0.74   0.45   0.84   0.78\n5      0      0   0.58   0.61   1.00   0.88\n6      0      0  -0.04   0.34   0.47   1.00\n\n# we use the data used in the list 4 correct recall\ndata_e1 &lt;- \n  data %&gt;% dplyr::select(`Condition`, `L4_RclP`)\n\n# Assuming your condition column is a factor, if not, convert it to factor\ndata_e1$Condition &lt;- factor(data_e1$Condition)\n\n# Summarize the data to get the average values and standard deviations for each condition\ndata_summary &lt;- data_e1 %&gt;%\n  group_by(Condition) %&gt;%\n  summarize(average = mean(L4_RclP),\n            sd = sd(L4_RclP))\n\n# create the plot\ndata_e1_plot &lt;- ggplot(data = data_summary,\n                       aes(x = average, y = Condition)) +\n                  geom_bar(stat = \"identity\", width = 0.25) + \n  scale_x_continuous(breaks = seq(0,0.8, by = 0.1)) + # set scale similar to original plot in paper\n                  geom_errorbar(aes(xmin = average - sd, xmax = average + sd), # calculate errorbars using sd\n                                width = 0.2,\n                                color = \"white\") +  # adjust color of error bars\n                  labs(title = \"Memory performance during List 4 recall in Experiment 1\",\n       x = \"Proportion of List 4 Correct Recall\",\n       y = NULL) + # Y-axis are self-explanatory\n  theme(plot.background = element_rect(fill = \"#2D3143\"), # region outside plot\n        panel.background = element_rect(fill = \"#2D3143\"), # region inside plot\n        panel.grid = element_line(color = \"darkgrey\"), # grid color\n        plot.title = element_text(color = \"white\"), # make text white in dark-mode\n        plot.subtitle = element_text(color = \"white\"), # make text white in dark-mode\n        axis.text.x = element_text(color = \"white\"), # make text white in dark-mode\n        axis.text.y = element_text(color = \"white\"), # make text white in dark-mode\n        axis.title.x.bottom = element_text(color = \"white\"), # make text white in dark-mode\n        axis.title.y.left = element_text(color = \"white\") # make text white in dark-mode\n        )\ndata_e1_plot\n\n\n\n\n\n\n\n\nTo compare our replicated plot to the original from the paper:\n\n\n\nimage\n\n\n\none.way &lt;- aov(L4_RclP ~ Condition, data = data_e1)\nsummary(one.way)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nCondition     2  2.634  1.3169   22.51 5.36e-09 ***\nResiduals   117  6.845  0.0585                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nReturns F(2, 117) = 22.51, p &lt; 0.001, whereas the original paper’s result is F(2, 117) = 22.57, p &lt; 0.001.\n\ndata_e1$Condition %&gt;% unique()\n\n[1] Always-Tested Early-Tested  Not-Tested   \nLevels: Always-Tested Early-Tested Not-Tested\n\nt.test(\n  L4_RclP ~ Condition, \n  data = data_e1, \n  subset = Condition == \"Always-Tested\" | Condition == \"Not-Tested\")\n\n\n    Welch Two Sample t-test\n\ndata:  L4_RclP by Condition\nt = 6.9752, df = 77.554, p-value = 8.992e-10\nalternative hypothesis: true difference in means between group Always-Tested and group Not-Tested is not equal to 0\n95 percent confidence interval:\n 0.2583119 0.4646881\nsample estimates:\nmean in group Always-Tested    mean in group Not-Tested \n                     0.7180                      0.3565 \n\n\nMean Always-Tested = 0.72 and Not-Tested 0.36 with t(78) = 6.98 , p &lt; 0.001\n\nt.test(\n  L4_RclP ~ Condition,\n  data = data_e1,\n  subset = Condition == \"Early-Tested\" | Condition == \"Not-Tested\")\n\n\n    Welch Two Sample t-test\n\ndata:  L4_RclP by Condition\nt = 2.7321, df = 77.484, p-value = 0.007792\nalternative hypothesis: true difference in means between group Early-Tested and group Not-Tested is not equal to 0\n95 percent confidence interval:\n 0.04156587 0.26493413\nsample estimates:\nmean in group Early-Tested   mean in group Not-Tested \n                   0.50975                    0.35650 \n\n\nMean Early-Tested = 0.51, t(78) = 2.73, p = 0.008\n\nt.test(\n  L4_RclP ~ Condition,\n  data = data_e1,\n  subset = Condition == \"Early-Tested\" | Condition == \"Always-Tested\")\n\n\n    Welch Two Sample t-test\n\ndata:  L4_RclP by Condition\nt = 3.8389, df = 76.136, p-value = 0.0002537\nalternative hypothesis: true difference in means between group Always-Tested and group Early-Tested is not equal to 0\n95 percent confidence interval:\n 0.1002101 0.3162899\nsample estimates:\nmean in group Always-Tested  mean in group Early-Tested \n                    0.71800                     0.50975 \n\n\nt(78) = 3.84, p &lt; 0,001\nIt is possible to replicate the data from the paper in some way however, not all results are identical. Providing the raw data, rather than the source code, leaves the person replicating the results open to the possibility of making different choices regarding the methods of analysis and is also prone to user-errors. One of the pillars of open science is indeed data, but so is code. And even relatively simple analyses can be interpreted differently by different people, while providing code provides a way to both reproduce the analyses but also provide a vector for other people to learn.",
    "crumbs": [
      "Home",
      "Open peer review"
    ]
  },
  {
    "objectID": "003_Guerrilla_analytics_framework.html",
    "href": "003_Guerrilla_analytics_framework.html",
    "title": "Guerrilla analytics",
    "section": "",
    "text": "Guerrilla analytics\nGuerrilla Analytics is data analytics performed in a very dynamic project environment that presents the team with varied and frequent disruptions and constrains the team in terms of the resources they can bring to bear on their analytics problem.  From: Guerrilla Analytics, 2015\nData science projects can get big, and keeping all files and folders tidy makes sure that it will stay possible to go through all the files and see how the analyses are performed. Keeping a well managed structure also allows for easier reproduction of the performed analyses when the folders are sent to other data scientist.  First we take a look at what a general folder structure might look like after a few weeks of assignments:\n\n\n\n\n\nOld folder structure:\n\n\nfs::dir_tree(path = “/Users/Alex/Documents/daur2/daurII_students_OLD”)  /Users/Alex/Documents/daur2/daurII_students_OLD  ├── Alex_Groot copy.csv  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.Rmd  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.html  ├── Andy.csv  ├── David.csv  ├── EXAM  │ ├── capstone_exam.Rmd  │ ├── capstone_exam.html  │ └── capstone_exam.md  ├── GDS858.soft.gz  ├── GPL11154.soft  ├── GPL96.soft  ├── GSE116936  │ └── GSE116936_RAW.tar  ├── GSE116936_series_matrix.txt.gz  ├── GSE116936_supp_files  │ ├── GSM3264576_A1-Ctrl.txt.gz  │ ├── GSM3264577_A2-Ctrl.txt.gz  │ ├── GSM3264578_A3-Ctrl.txt.gz  │ ├── GSM3264579_B1-PGE2.txt.gz  │ ├── GSM3264580_B2-PGE2.txt.gz  │ ├── GSM3264581_B3-PGE2.txt.gz  │ ├── GSM3264582_I1-TNF.txt.gz  │ ├── GSM3264583_I2-TNF.txt.gz  │ ├── GSM3264584_I3-TNF.txt.gz  │ ├── GSM3264585_J1-TNF+PGE2.txt.gz  │ ├── GSM3264586_J2-TNF+PGE2.txt.gz  │ └── GSM3264587_J3-TNF+PGE2.txt.gz  ├── GSE148829  │ ├── GSE148829_BEAS_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_Human1_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_Human2_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_Human_Ileum_absorptiveAndCryptentero_dge.csv.gz  │ ├── GSE148829_Human_lung_epithelial_cell_raw_counts.txt.gz  │ ├── GSE148829_Mouse_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_NHP_TB_Grans_and_Lung_dge.txt.gz  │ ├── GSE148829_NHP_ileum_epithelial_raw_expression_counts.csv.gz  │ ├── GSE148829_NHP_lung_epithelial_raw_expression_counts.csv.gz  │ ├── GSE148829_NasalSSS_cleaned_raw_expression_counts.csv.gz  │ ├── GSE148829_RAW  │ │ ├── GSM4487938_Mouse_Nasal_IFNa_NH11_dge.txt.gz  │ │ ├── GSM4487939_Mouse_Nasal_IFNa_NH12_dge.txt.gz  │ │ ├── GSM4487940_Mouse_Nasal_IFNa_NH21_dge.txt.gz  │ │ ├── GSM4487941_Mouse_Nasal_IFNa_NH22_dge.txt.gz  │ │ ├── GSM4487942_Mouse_Nasal_IFNa_NS11_dge.txt.gz  │ │ ├── GSM4487943_Mouse_Nasal_IFNa_NS12_dge.txt.gz  │ │ ├── GSM4487944_Mouse_Nasal_IFNa_NS21_dge.txt.gz  │ │ ├── GSM4487945_Mouse_Nasal_IFNa_NS22_dge.txt.gz  │ │ ├── GSM4488219_Mouse_basal_stim_IFNA_0.1_REP1_TPM.txt.gz  │ │ ├── GSM4488220_Mouse_basal_stim_IFNA_0.1_REP2_TPM.txt.gz  │ │ ├── GSM4488221_Mouse_basal_stim_IFNA_0.1_REP3_TPM.txt.gz  │ │ ├── GSM4488222_Mouse_basal_stim_IFNA_0.5_REP1_TPM.txt.gz  │ │ ├── GSM4488223_Mouse_basal_stim_IFNA_0.5_REP2_TPM.txt.gz  │ │ ├── GSM4488224_Mouse_basal_stim_IFNA_0.5_REP3_TPM.txt.gz  │ │ ├── GSM4488225_Mouse_basal_stim_IFNB_0.1_REP1_TPM.txt.gz  │ │ ├── GSM4488226_Mouse_basal_stim_IFNB_0.1_REP2_TPM.txt.gz  │ │ ├── GSM4488227_Mouse_basal_stim_IFNB_0.1_REP3_TPM.txt.gz  │ │ ├── GSM4488228_Mouse_basal_stim_IFNB_0.5_REP1_TPM.txt.gz  │ │ ├── GSM4488229_Mouse_basal_stim_IFNB_0.5_REP2_TPM.txt.gz  │ │ ├── GSM4488230_Mouse_basal_stim_IFNB_0.5_REP3_TPM.txt.gz  │ │ ├── GSM4488231_Mouse_basal_stim_IFNA_1_REP1_TPM.txt.gz  │ │ ├── GSM4488232_Mouse_basal_stim_IFNA_1_REP2_TPM.txt.gz  │ │ ├── GSM4488233_Mouse_basal_stim_IFNA_1_REP3_TPM.txt.gz  │ │ ├── GSM4488234_Mouse_basal_stim_IFNA_2_REP1_TPM.txt.gz  │ │ ├── GSM4488235_Mouse_basal_stim_IFNA_2_REP2_TPM.txt.gz  │ │ ├── GSM4488236_Mouse_basal_stim_IFNA_2_REP3_TPM.txt.gz  │ │ ├── GSM4488237_Mouse_basal_stim_IFNB_1_REP1_TPM.txt.gz  │ │ ├── GSM4488238_Mouse_basal_stim_IFNB_1_REP2_TPM.txt.gz  │ │ ├── GSM4488239_Mouse_basal_stim_IFNB_1_REP3_TPM.txt.gz  │ │ ├── GSM4488240_Mouse_basal_stim_IFNB_2_REP1_TPM.txt.gz  │ │ ├── GSM4488241_Mouse_basal_stim_IFNB_2_REP2_TPM.txt.gz  │ │ ├── GSM4488242_Mouse_basal_stim_IFNB_2_REP3_TPM.txt.gz  │ │ ├── GSM4488243_Mouse_basal_stim_IFNA_5_REP1_TPM.txt.gz  │ │ ├── GSM4488244_Mouse_basal_stim_IFNA_5_REP2_TPM.txt.gz  │ │ ├── GSM4488245_Mouse_basal_stim_IFNA_5_REP3_TPM.txt.gz  │ │ ├── GSM4488246_Mouse_basal_stim_IFNA_10_REP1_TPM.txt.gz  │ │ ├── GSM4488247_Mouse_basal_stim_IFNA_10_REP2_TPM.txt.gz  │ │ ├── GSM4488248_Mouse_basal_stim_IFNA_10_REP3_TPM.txt.gz  │ │ ├── GSM4488249_Mouse_basal_stim_IFNB_5_REP1_TPM.txt.gz  │ │ ├── GSM4488250_Mouse_basal_stim_IFNB_5_REP2_TPM.txt.gz  │ │ ├── GSM4488251_Mouse_basal_stim_IFNB_5_REP3_TPM.txt.gz  │ │ ├── GSM4488252_Mouse_basal_stim_IFNB_10_REP1_TPM.txt.gz  │ │ ├── GSM4488253_Mouse_basal_stim_IFNB_10_REP2_TPM.txt.gz  │ │ ├── GSM4488254_Mouse_basal_stim_IFNB_10_REP3_TPM.txt.gz  │ │ ├── GSM4488255_Mouse_basal_stim_IFNG_0.1_REP1_TPM.txt.gz  │ │ ├── GSM4488256_Mouse_basal_stim_IFNG_0.1_REP2_TPM.txt.gz  │ │ ├── GSM4488257_Mouse_basal_stim_IFNG_0.1_REP3_TPM.txt.gz  │ │ ├── GSM4488258_Mouse_basal_stim_IFNG_0.5_REP1_TPM.txt.gz  │ │ ├── GSM4488259_Mouse_basal_stim_IFNG_0.5_REP2_TPM.txt.gz  │ │ ├── GSM4488260_Mouse_basal_stim_IFNG_0.5_REP3_TPM.txt.gz  │ │ ├── GSM4488261_Mouse_basal_stim_IFNG_1_REP1_TPM.txt.gz  │ │ ├── GSM4488262_Mouse_basal_stim_IFNG_1_REP2_TPM.txt.gz  │ │ ├── GSM4488263_Mouse_basal_stim_IFNG_1_REP3_TPM.txt.gz  │ │ ├── GSM4488264_Mouse_basal_stim_IFNG_2_REP1_TPM.txt.gz  │ │ ├── GSM4488265_Mouse_basal_stim_IFNG_2_REP2_TPM.txt.gz  │ │ ├── GSM4488266_Mouse_basal_stim_IFNG_2_REP3_TPM.txt.gz  │ │ ├── GSM4488267_Mouse_basal_stim_IFNG_5_REP1_TPM.txt.gz  │ │ ├── GSM4488268_Mouse_basal_stim_IFNG_5_REP2_TPM.txt.gz  │ │ ├── GSM4488269_Mouse_basal_stim_IFNG_5_REP3_TPM.txt.gz  │ │ ├── GSM4488270_Mouse_basal_stim_IFNG_10_REP1_TPM.txt.gz  │ │ ├── GSM4488271_Mouse_basal_stim_IFNG_10_REP2_TPM.txt.gz  │ │ ├── GSM4488272_Mouse_basal_stim_IFNG_10_REP3_TPM.txt.gz  │ │ ├── GSM4488273_Mouse_basal_stim_Untreated_0_REP1_TPM.txt.gz  │ │ ├── GSM4488274_Mouse_basal_stim_Untreated_0_REP2_TPM.txt.gz  │ │ ├── GSM4488275_Mouse_basal_stim_Untreated_0_REP3_TPM.txt.gz  │ │ ├── GSM4488276_Mouse_basal_stim_Untreated_0_REP4_TPM.txt.gz  │ │ ├── GSM4488277_Mouse_basal_stim_Untreated_0_REP5_TPM.txt.gz  │ │ ├── GSM4488278_Mouse_basal_stim_Untreated_0_REP6_TPM.txt.gz  │ │ ├── GSM4488279_Mouse_basal_stim_Untreated_0_REP7_TPM.txt.gz  │ │ ├── GSM4488280_Mouse_basal_stim_Untreated_0_REP8_TPM.txt.gz  │ │ ├── GSM4488281_Mouse_basal_stim_Untreated_0_REP9_TPM.txt.gz  │ │ ├── GSM4488282_Mouse_basal_stim_Untreated_0_REP10_TPM.txt.gz  │ │ ├── GSM4488283_Mouse_basal_stim_Untreated_0_REP11_TPM.txt.gz  │ │ └── GSM4488284_Mouse_basal_stim_Untreated_0_REP12_TPM.txt.gz  │ └── GSE148829_RAW.tar  ├── GSE75073  │ ├── GSE75073_RAW.tar  │ ├── GSE75073_Signals_450k.txt.gz  │ └── GSE75073_Signals_Epic.txt.gz  ├── GSE76073  │ ├── GSE76073_Normalized_counts.txt.gz  │ └── GSE76073_Raw_counts.txt.gz  ├── GSE923_series_matrix.txt.gz  ├── Images  │ ├── 2020-04-20CORT.png  │ ├── 20200403_citrul.png  │ ├── 20200404_citrul.png  │ ├── 20200404_ifabp.png  │ ├── Bioconductor.png  │ ├── Biostrings.png  │ ├── BrowseSeq  │ │ ├── Dia1.PNG  │ │ └── sequence_alignment_plots  │ │ ├── Dia1.PNG  │ │ ├── Dia2.PNG  │ │ ├── Dia3.PNG  │ │ ├── Dia4.PNG  │ │ ├── Dia5.PNG  │ │ ├── Dia6.PNG  │ │ ├── Dia7.PNG  │ │ └── Dia8.PNG  │ ├── DESeq2.png  │ ├── Dia1.JPG  │ ├── Dia3.JPG  │ ├── GenomicRanges.png  │ ├── Git_Setup_1.JPG  │ ├── Glimma.png  │ ├── IRanges.png  │ ├── MSnbase.png  │ ├── Project_1.jpg  │ ├── Project_2.jpg  │ ├── Rlogo.png  │ ├── Rtools.png  │ ├── bioconductorlogo.jpg  │ ├── cat_sleeping_on_keyboard.jpg  │ ├── citrulline.png  │ ├── connectome-1.png  │ ├── cortline.png  │ ├── covid19_ziekenhuis.png  │ ├── dotdotdot_rotate_axis.png  │ ├── edgeR.png  │ ├── exercise_2_1_1.png  │ ├── exercise_2_1_1_c.png  │ ├── forcats.png  │ ├── git_web.png  │ ├── gitinstallermac.png  │ ├── heatmap.png  │ ├── heatmap1.png  │ ├── heatmap2.png  │ ├── hex-ggplot2.png  │ ├── installpackages.png  │ ├── loadpackages.png  │ ├── logo_R.jpg  │ ├── ma1.png  │ ├── ma2.png  │ ├── mzR_hl.png  │ ├── new_proj_done.png  │ ├── new_proj_git.png  │ ├── one-doesnot-simply.jpg  │ ├── open_486.jpg  │ ├── open_compaq.jpg  │ ├── pca1.png  │ ├── pepper.jpg  │ ├── pipe.png  │ ├── purrr_small.png  │ ├── purrr_sticker.jpg  │ ├── pvalues.png  │ ├── r4ds.png  │ ├── readr_sticker.png  │ ├── readxl_sticker.png  │ ├── rmac_download.png  │ ├── rstudio_download.png  │ ├── rstudio_leeg.png  │ ├── rwin_download.png  │ ├── security.png  │ ├── setup-wizard-rstudio.png  │ ├── setupRtools.png  │ ├── stringr.png  │ ├── summarizedexperiment.png  │ ├── tidyr_sticker.png  │ ├── tidyverse_sticker.png  │ ├── usrlocal.png  │ ├── wizardRmac.png  │ ├── wizardRwindows.png  │ ├── wontinstall.png  │ ├── workflow_course.png  │ ├── xquarkdisk.png  │ ├── xquarkwizard.png  │ ├── xquartz.png  │ └── yoda_the_last_jedi.jpg  ├── John.csv  ├── Les 6.Rmd  ├── Les4.Rmd  ├── Les4.nb.html  ├── Les5.Rmd  ├── Lesson_5_Exercise_a1.Rmd  ├── Mike.csv  ├── R  │ └── do_tidy_pertussis.R  ├── README.md  ├── Steve.csv  ├── answers  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.Rmd  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.html  │ ├── daur2_lesson51_demo_multiple_sequence_alignment_files  │ │ └── figure-html  │ │ ├── unnamed-chunk-23-1.png  │ │ ├── unnamed-chunk-24-1.png  │ │ ├── unnamed-chunk-29-1.png  │ │ └── unnamed-chunk-30-1.png  │ ├── daur2_lesson_6_exercise_rnaseq.Rmd  │ ├── daur2_lesson_6_exercise_rnaseq.html  │ ├── daur2_lesson_6_exercise_rnaseq_student_version.Rmd  │ └── daur2_lesson_6_exercise_rnaseq_student_version.html  ├── caspase_align.aux  ├── caspase_align.log  ├── caspase_align.tex  ├── code  │ └── load_data.R  ├── data  │ ├── CISID_pertussis_10082018.csv  │ ├── biomaRt_homology_example.rds  │ ├── biomaRt_homology_example_aa.rds  │ ├── biomaRt_homology_example_dna.rds  │ ├── chircus_blg_aminoacids.fasta  │ ├── course_r_packages.tsv  │ ├── course_r_packages.txt  │ ├── covid_rivm  │ ├── go_bp-hit.csv  │ ├── go_cc-hit.csv  │ ├── go_mf-hit.csv  │ ├── hobbit_chapter.txt  │ ├── hobbit_dwarves.txt  │ ├── hox_aa_sequences.txt  │ ├── hox_dna_sequences.txt  │ ├── human_hoxb1_protein.fasta  │ ├── human_hoxb1_protein.gb  │ ├── human_rhodopsin_protein.fasta  │ ├── human_ttn_genomic_dna.fasta  │ ├── jp_coldata.txt  │ ├── jp_counts.txt  │ ├── jp_features.txt  │ ├── jp_miame.rds  │ ├── jp_miame.txt  │ ├── lesson2  │ │ ├── ENST00000642929.1.fa  │ │ ├── ENST00000643997.1.fa  │ │ ├── ENST00000644264.1.fa  │ │ ├── ENST00000644696.1.fa  │ │ ├── ENST00000644919.1.fa  │ │ ├── ENST00000646017.1.fa  │ │ ├── ENST00000646618.2.fa  │ │ ├── ENST00000646644.1.fa  │ │ ├── ENST00000647220.1.fa  │ │ ├── ENST00000647443.1.fa  │ │ ├── cholesterol.xlsx  │ │ ├── diet  │ │ │ ├── Andy.csv  │ │ │ ├── David.csv  │ │ │ ├── John.csv  │ │ │ ├── Mike.csv  │ │ │ └── Steve.csv  │ │ ├── diet_data.zip  │ │ └── sequence1.txt  │ └── lesson3  │ ├── GO-terms.txt  │ ├── dna_random  │ ├── sequence.fasta  │ ├── sequence1.txt  │ └── sequence2.txt  ├── daurII_students  │ ├── R  │ │ └── do_tidy_pertussis.R  │ ├── README.md  │ ├── code  │ │ └── load_data.R  │ ├── data  │ │ ├── CISID_pertussis_10082018.csv  │ │ ├── course_r_packages.tsv  │ │ ├── covid_rivm  │ │ ├── go_bp-hit.csv  │ │ ├── go_cc-hit.csv  │ │ ├── go_mf-hit.csv  │ │ ├── hobbit_chapter.txt  │ │ ├── hobbit_dwarves.txt  │ │ ├── hox_aa_sequences.txt  │ │ ├── hox_dna_sequences.txt  │ │ ├── human_hoxb1_protein.fasta  │ │ ├── human_hoxb1_protein.gb  │ │ ├── human_rhodopsin_protein.fasta  │ │ ├── human_ttn_genomic_dna.fasta  │ │ └── lesson2  │ │ └── diet_data.zip  │ ├── daurII_students.Rproj  │ └── demos  │ └── 01_demo_functions.Rmd  ├── daurII_students.Rproj  ├── demos  │ ├── 01_demo_functions.Rmd  │ ├── 021_demo_2_agenda.Rmd  │ ├── 021_demo_2_agenda.html  │ ├── 022_lesson2_map_nest.Rmd  │ ├── 022_lesson2_map_nest.html  │ ├── 031_strings_and_regex.Rmd  │ ├── 031_strings_and_regex.html  │ ├── 031_strings_and_regex.pdf  │ ├── 041_biomart.Rmd  │ ├── 042_geoquery.Rmd  │ ├── 043_rentrez.Rmd  │ ├── 051_demo_multiple_sequence_alignment.Rmd  │ ├── 051_demo_multiple_sequence_alignment.html  │ ├── 051_demo_multiple_sequence_alignment.md  │ ├── 051_demo_multiple_sequence_alignment_files  │ │ └── figure-html  │ │ ├── unnamed-chunk-23-1.png  │ │ ├── unnamed-chunk-24-1.png  │ │ ├── unnamed-chunk-29-1.png  │ │ └── unnamed-chunk-30-1.png  │ ├── 052_demo_summarized_experiment.Rmd  │ ├── 052_demo_summarized_experiment.html  │ ├── 052_demo_summarized_experiment.md  │ ├── 61_heatmaps.Rmd  │ ├── 61_heatmaps.html  │ ├── 62_rnseq_workflow.Rmd  │ └── 62_rnseq_workflow.html  ├── les1.Rmd  ├── les1.nb.html  ├── les2.Rmd  ├── les2.nb.html  ├── les3.Rmd  ├── my_transcript.fasta  ├── resources.Rmd  └── unzip  ├── Andy.csv  ├── David.csv  ├── John.csv  ├── Mike.csv  └── Steve.csv \n\n\n  As you can see, the original file structure used during the earlier data science course is quite the mess. Instead we now make usage of the Guerilla analytics framework to store data and manage files. Using simple structures to store data, code, generated graphs and makes it easier to use and replicate analyses, even years after the analyses are performed.  Making sure all files in a environment pertain only to that single project is important. And to make data storage simple, a central data folder in the root of the enviornment is used with clear naming conventions, dates and versions. Also metadata files are used to allow anyone with access to the project to see what the contents of the project are.  Here is the above file structure, with the guerilla analytics framework retroactively applied: \n\n\nNew folder structure:\n\n\n├── EXAM  │ ├── Alex_Groot copy.csv  │ ├── capstone_exam.Rmd  │ ├── capstone_exam.html  │ └── capstone_exam.md  ├── Images  │ ├── 2020-04-20CORT.png  │ ├── 20200403_citrul.png  │ ├── BrowseSeq  │ │ ├── Dia1.PNG  │ │ └── sequence_alignment_plots  │ │ ├── Dia1.PNG  │ │ ├── Dia2.PNG  │ └── yoda_the_last_jedi.jpg  ├── R  │ └── do_tidy_pertussis.R  ├── README.md  ├── caspase_align.aux  ├── caspase_align.log  ├── caspase_align.tex  ├── code  │ └── load_data.R  ├── data_raw  │ ├── lesson2  │ │ ├── ENST00000642929.1.fa  │ │ ├── ENST00000643997.1.fa  │ │ ├── ENST00000644264.1.fa  │ │ ├── cholesterol.xlsx  │ │ ├── diet  │ │ │ ├── Andy.csv  │ │ │ └── Steve.csv  │ │ ├── diet_data.zip  │ │ └── sequence1.txt  │ ├── lesson3  │ │ ├── GO-terms.txt  │ │ ├── dna_random  │ │ ├── sequence.fasta  │ │ ├── sequence1.txt  │ │ └── sequence2.txt  │ └── unsorted  │ ├── Andy.csv  │ ├── CISID_pertussis_10082018.csv  │ ├── David.csv  │ ├── GSE116936  │ │ └── GSE116936_RAW.tar  │ ├── GSE116936_series_matrix.txt.gz  │ ├── GSE116936_supp_files  │ │ ├── GSM3264576_A1-Ctrl.txt.gz  │ │ ├── GSM3264577_A2-Ctrl.txt.gz  │ │ ├── GSM3264578_A3-Ctrl.txt.gz  │ │ ├── GSM3264585_J1-TNF+PGE2.txt.gz  │ │ ├── GSM3264586_J2-TNF+PGE2.txt.gz  │ │ └── GSM3264587_J3-TNF+PGE2.txt.gz  │ ├── GSE148829  │ │ ├── GSE148829_BEAS_Basal_Pops_TPM.txt.gz  │ │ ├── GSE148829_Human1_Basal_Pops_TPM.txt.gz  │ │ ├── GSE148829_Human2_Basal_Pops_TPM.txt.gz  │ │ │ ├── GSM4487938_Mouse_Nasal_IFNa_NH11_dge.txt.gz  │ │ │ ├── GSM4487939_Mouse_Nasal_IFNa_NH12_dge.txt.gz  │ │ │ ├── GSM4487940_Mouse_Nasal_IFNa_NH21_dge.txt.gz  │ │ │ ├── GSM4487941_Mouse_Nasal_IFNa_NH22_dge.txt.gz  │ │ │ ├── GSM4487942_Mouse_Nasal_IFNa_NS11_dge.txt.gz  │ │ │ ├── GSM4487943_Mouse_Nasal_IFNa_NS12_dge.txt.gz  │ │ │ ├── GSM4487944_Mouse_Nasal_IFNa_NS21_dge.txt.gz  │ │ │ ├── GSM4487945_Mouse_Nasal_IFNa_NS22_dge.txt.gz  │ │ │ ├── GSM4488282_Mouse_basal_stim_Untreated_0_REP10_TPM.txt.gz  │ │ │ ├── GSM4488283_Mouse_basal_stim_Untreated_0_REP11_TPM.txt.gz  │ │ │ └── GSM4488284_Mouse_basal_stim_Untreated_0_REP12_TPM.txt.gz  │ │ └── GSE148829_RAW.tar  │ ├── GSE75073  │ │ ├── GSE75073_RAW.tar  │ │ ├── GSE75073_Signals_450k.txt.gz  │ │ └── GSE75073_Signals_Epic.txt.gz  │ ├── GSE76073  │ │ ├── GSE76073_Normalized_counts.txt.gz  │ │ └── GSE76073_Raw_counts.txt.gz  │ ├── GSE923_series_matrix.txt.gz  │ ├── human_hoxb1_protein.fasta  │ ├── human_hoxb1_protein.gb  │ ├── human_rhodopsin_protein.fasta  │ ├── human_ttn_genomic_dna.fasta  │ ├── jp_coldata.txt  │ ├── jp_counts.txt  │ ├── jp_features.txt  │ ├── jp_miame.txt  │ ├── my_transcript.fasta  │ └── unzip  │ ├── Andy.csv  │ ├── David.csv  │ ├── John.csv  │ ├── Mike.csv  │ └── Steve.csv  ├── data_rds  │ ├── biomaRt_homology_example.rds  │ ├── biomaRt_homology_example_aa.rds  │ ├── biomaRt_homology_example_dna.rds  │ └── jp_miame.rds  ├── daurII_students  │ ├── R  │ │ └── do_tidy_pertussis.R  │ ├── README.md  │ ├── code  │ │ └── load_data.R  │ ├── data  │ │ ├── CISID_pertussis_10082018.csv  │ │ ├── course_r_packages.tsv  │ │ ├── covid_rivm  │ │ ├── human_rhodopsin_protein.fasta  │ │ ├── human_ttn_genomic_dna.fasta  │ │ └── lesson2  │ │ └── diet_data.zip  │ ├── daurII_students.Rproj  │ └── demos  │ └── 01_demo_functions.Rmd  ├── daurII_students.Rproj  ├── demos  │ ├── 01_demo_functions.Rmd  │ ├── 021_demo_2_agenda.Rmd  │ ├── 021_demo_2_agenda.html  │ ├── 61_heatmaps.Rmd  │ ├── 61_heatmaps.html  │ ├── 62_rnseq_workflow.Rmd  │ └── 62_rnseq_workflow.html  └── markdown  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.Rmd  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.html  ├── Les 6.Rmd  ├── Les4.Rmd  ├── Les4.nb.html  ├── Les5.Rmd  ├── Lesson_5_Exercise_a1.Rmd  ├── answers  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.Rmd  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.html  │ ├── daur2_lesson51_demo_multiple_sequence_alignment_files  │ │ └── figure-html  │ │ ├── unnamed-chunk-23-1.png  │ │ ├── unnamed-chunk-24-1.png  │ │ ├── unnamed-chunk-29-1.png  │ │ └── unnamed-chunk-30-1.png  │ ├── daur2_lesson_6_exercise_rnaseq.Rmd  │ ├── daur2_lesson_6_exercise_rnaseq.html  │ ├── daur2_lesson_6_exercise_rnaseq_student_version.Rmd  │ └── daur2_lesson_6_exercise_rnaseq_student_version.html  ├── les1.Rmd  ├── les1.nb.html  ├── les2.Rmd  ├── les2.nb.html  ├── les3.Rmd  └── resources.Rmd  \n\n\n  In order to have the folder structured in the Guerrilla analytics framework method, I retroactively moved the files. This is just used as an example to show what a tidier and better organized structure might look like. The big problem with this is that it will  break many, if not all, of the scripts inside due to changing of file paths.\nThe best way to keep files well-managed is to set up a file structure before starting and sticking to it. Keeping a folder with all the raw data with its names the way they were obtained makes sure confusion is kept to a minimum when the data is looked at. Creating seperate data folders enables to track transformed data while preserving the raw data.\nAnd keeping metadata files in all the folders that explain not just folder structure, but importantly also file naming conventions and potentially used abbreviations for them will mean that even years later someone will be able to look at the project and learn what is inside.\nThis portfolio was made with the Guerrilla analytics framework in mind from the start:",
    "crumbs": [
      "Home",
      "Guerrilla analytics"
    ]
  },
  {
    "objectID": "010_ML_Getting_started.html",
    "href": "010_ML_Getting_started.html",
    "title": "Getting started with machine learning",
    "section": "",
    "text": "Getting started with machine learning\nMachine learning can be split between supervised learning and unsupervised learning. Supervised learning requires labelled input and unsupervised learning does not require labelled input data. Supervised learning is best used for classification and regression-based models while unsupervised is best for clustering and association rules.\nIn this project we make use of a Tidymodels regression classification supervised learning model. The breast cancer dataset contains patient data with attributes. With Tidymodels we utilize attributes, such as age, tumor size and degree of malignance to predict recurrence events.\nModel fitting: A simple example of fitting a model would be having the function for a line ‘y = mx + b’ and finding the values for m and b that get the outcome of y, which corresponds with the training data. Basically input data, analyze the input and retrieve the function to recreate the line with different datasets.\nHyperparameter tuning; Fit 90% of your data and test the remaining 10% test fold. Repeat for each separate fold and pick the best results.\n\nLoad R packages\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tibble)\ntidymodels_prefer() # prefer to use functions from tidymodels package\nlibrary(ggplot2)\nlibrary(parsnip)\nlibrary(pROC)\nlibrary(stringr)\n\n\n\nLoad data\n\ndata_raw &lt;- as_tibble(read.csv(here::here(\"data_raw/data_00100/breast_cancer/breast-cancer.data\"), header = FALSE))\ndata_lines &lt;- grep(\"\\\\s+[0-9]+\\\\.\\\\s+[A-Za-z-]+:\", \n                   readLines(here::here(\"data_raw/data_00100/breast_cancer/breast-cancer.names\")), value = TRUE)\ndata_names &lt;- sub(\"\\\\s+[0-9]+\\\\.\\\\s+([A-Za-z-]+):.*\", \"\\\\1\", data_lines)[1:10]\ndata_names &lt;- data_names %&gt;% gsub(pattern = \"-\", replacement = \"_\")\n\ndata_tbl &lt;- stats::setNames(data_raw, data_names)\n\n\n\nInspect variables\n\nhead(data_tbl) %&gt;%\n  kableExtra::kbl()\n\n\n\n\nClass\nage\nmenopause\ntumor_size\ninv_nodes\nnode_caps\ndeg_malig\nbreast\nbreast_quad\nirradiat\n\n\n\n\nno-recurrence-events\n30-39\npremeno\n30-34\n0-2\nno\n3\nleft\nleft_low\nno\n\n\nno-recurrence-events\n40-49\npremeno\n20-24\n0-2\nno\n2\nright\nright_up\nno\n\n\nno-recurrence-events\n40-49\npremeno\n20-24\n0-2\nno\n2\nleft\nleft_low\nno\n\n\nno-recurrence-events\n60-69\nge40\n15-19\n0-2\nno\n2\nright\nleft_up\nno\n\n\nno-recurrence-events\n40-49\npremeno\n0-4\n0-2\nno\n2\nright\nright_low\nno\n\n\nno-recurrence-events\n60-69\nge40\n15-19\n0-2\nno\n2\nleft\nleft_low\nno\n\n\n\n\n\n\n# # TODO # create concat list\nsummary(data_tbl)\n\n    Class               age             menopause          tumor_size       \n Length:286         Length:286         Length:286         Length:286        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  inv_nodes          node_caps           deg_malig        breast         \n Length:286         Length:286         Min.   :1.000   Length:286        \n Class :character   Class :character   1st Qu.:2.000   Class :character  \n Mode  :character   Mode  :character   Median :2.000   Mode  :character  \n                                       Mean   :2.049                     \n                                       3rd Qu.:3.000                     \n                                       Max.   :3.000                     \n breast_quad          irradiat        \n Length:286         Length:286        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\nsum(is.na(data_tbl))\n\n[1] 0\n\n# Change chr to factor\ndata_cols &lt;- colnames(data_tbl[c(\"Class\", \"menopause\", \"node_caps\", \"breast\", \"breast_quad\", \"irradiat\")])\ndata_tbl[data_cols] &lt;- lapply(data_tbl[data_cols], factor)\ndata_tbl\n\n# A tibble: 286 × 10\n   Class         age   menopause tumor_size inv_nodes node_caps deg_malig breast\n   &lt;fct&gt;         &lt;chr&gt; &lt;fct&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; \n 1 no-recurrenc… 30-39 premeno   30-34      0-2       no                3 left  \n 2 no-recurrenc… 40-49 premeno   20-24      0-2       no                2 right \n 3 no-recurrenc… 40-49 premeno   20-24      0-2       no                2 left  \n 4 no-recurrenc… 60-69 ge40      15-19      0-2       no                2 right \n 5 no-recurrenc… 40-49 premeno   0-4        0-2       no                2 right \n 6 no-recurrenc… 60-69 ge40      15-19      0-2       no                2 left  \n 7 no-recurrenc… 50-59 premeno   25-29      0-2       no                2 left  \n 8 no-recurrenc… 60-69 ge40      20-24      0-2       no                1 left  \n 9 no-recurrenc… 40-49 premeno   50-54      0-2       no                2 left  \n10 no-recurrenc… 40-49 premeno   20-24      0-2       no                2 right \n# ℹ 276 more rows\n# ℹ 2 more variables: breast_quad &lt;fct&gt;, irradiat &lt;fct&gt;\n\n# change chr to numeric\nrange_to_numeric &lt;- function(column) {\n  column &lt;- str_extract(column, \"\\\\d+-\\\\d+\") # Extract range\n  column &lt;- (as.numeric(str_extract(column, \"^\\\\d+\")) +\n               as.numeric(str_extract(column, \"\\\\d+$\"))) / 2 # Calc middle\n}\n\ndata_input &lt;- data_tbl %&gt;%\n  mutate(across(c(age, tumor_size, inv_nodes), range_to_numeric))\n\n# rename Class to recurrence\nnames(data_input)[1] &lt;- \"recurrence\"\n\ndata_input %&gt;%\n  ggplot(aes(x = recurrence)) +\n  geom_bar() +\n  labs(title = \"Recurrence events\")\n\n\n\n\n\n\n\ndata_input %&gt;%\n  ggplot(aes(x = age)) +\n  geom_histogram(colour = \"black\",\n                 fill = \"grey\",\n                 binwidth = 9) +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\n\n\n\nSplit into training and testing\n\nset.seed(20241115) # current date\nprop_split &lt;- 0.75\nhdata_split &lt;- initial_split(data_input, \n                             prop = prop_split)\nhdata_train &lt;- training(hdata_split) # extract split training data\nhdata_test &lt;- testing(hdata_split) # extract split testing data\n\nWe set a seed for the sake of reproducibility. For ease of use todays date is used. We then set the initial split of the data with a proportion. The dataset contains 286 patients across varying ages. We wouldn’t want to set the split too high or too low and skew the data, therefore we opt for a proportion of 0.75 to start.\n\n\nCreate cross validation folds\nWe perform the vfold cross validation:\n\nhdata_folds &lt;- vfold_cv(hdata_train, v = 10)\n\nThe training dataset is selected and is split into 10 groups of equal size or “folds”.\n\n\nBuild a recipe\nWe want to model the outcome, recurrence, by modeling all potential variables in the dataset so we use a “.”, you could also age + deg_malig + recurrence, for example.\nA dummy variable is a binary variable used in modeling algorithms to represent the presence or absence of a categorical variable, particularly when calculations require numerical data. - https://www.sciencedirect.com/topics/computer-science/dummy-variable\nIn order to get the best results we normalize the data. Some observations have way higher values in the thousands, whilst others have values that are 0 or 1.\n\n# y ~ x; recurrence ~ allvariables\nhdata_recipe &lt;- recipe(recurrence ~ ., data = hdata_train) %&gt;%\n  # Dummy encoding for nominal variables\n  step_dummy(menopause, node_caps, breast, breast_quad, irradiat) %&gt;%\n  # Normalize numeric variables\n  step_normalize(age, tumor_size, inv_nodes, deg_malig)\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(hdata_recipe)\n\n\n# y ~ x; recurrence ~ allvariables\nhdata_recipe &lt;- recipe(recurrence ~ ., data = hdata_train) %&gt;%\n  step_dummy(sex) %&gt;% step_normalize(age, tumor_size, inv_nodes, deg_malig)\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(hdata_recipe)\n\nLASSO regression is used to help select the explanatory variables to include.\nModel evaluation: Accuracy: Proportion of data that are predicted correctly. Because it looks at proportion in the way it does it is not as useful for rare cases where a small set of a population is analyzed, e.g. &lt;1% of people have a certain condition, you could say no one has that certain condition and you’d be 99% correct but still doing poorly. ROC AUC: Area under curve\n\ntune_spec_lasso &lt;-\n  logistic_reg(penalty = tune(), # penalty is the selected lambda for the LASSO regression, tune() makes it\n               mixture = 1) %&gt;% # mixture = 1 = LASSO regression, 0 = ridge regression model\n  set_engine(\"glmnet\")\n\nModels can be used for classification or prediction The default for logistic regression is classification. The penalty argument specifies the value for lambda in the LASSO, we use the tune() function instead of picking a specific value. Setting the mixture to 1 sets the function to LASSO regression while 0 sets it to ridge regression. set_engine sets which r package performs the LASSO regression, the package used in this case is glmnet.  All other available engines are:\n\nknitr::kable(show_engines(\"linear_reg\"))\n\n\n\n\nengine\nmode\n\n\n\n\nlm\nregression\n\n\nglm\nregression\n\n\nglmnet\nregression\n\n\nstan\nregression\n\n\nspark\nregression\n\n\nkeras\nregression\n\n\nbrulee\nregression\n\n\n\n\n\nNext we need to tune the model, fit and choose the best one using tune_grid().  We need to specify 3 things:  What model are we choosing  What data are we fitting it to  What values of lambda do we want to try\n\nlasso_grid &lt;- tune_grid(\n  add_model(wf, tune_spec_lasso),\n  resamples = hdata_folds,\n  grid = grid_regular(penalty(), levels = 30) # 30 different equally spaced lambda\n)\n\nhighest_roc_auc_lasso &lt;- lasso_grid %&gt;% select_best(metric = \"roc_auc\")\nhighest_roc_auc_lasso\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0418 Preprocessor1_Model26\n\n\nWe add wf, our workflow containing the recipe, the variables we’re including and pre-processing steps and the model tune_spec_lasso. We want to fit our model to our cross-validation data hdata_folds and the grid contains the list of lambda values we want to try. We use grid_regular to try different values that are regularly spaced, containing penalty(), which has to match up with the penalty() from tune_spec_lasso. And then we add levels = 30 for the amount of different levels of equally spaced values of lambda.\nWe want the output of lasso_grid to then evaluate and select “the best” lambda. The metric we use here is the highest roc under the curve and we save this output as highest_roc_auc_lasso, which we will use to fit our final model.\n\nfinal_lasso &lt;- finalize_workflow(add_model(wf, tune_spec_lasso), highest_roc_auc_lasso)\n\nfinal_lasso\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.0417531893656041\n  mixture = 1\n\nComputational engine: glmnet \n\nlast_fit_metrics &lt;- last_fit(final_lasso, hdata_split) %&gt;%\n  collect_metrics()\n\nlast_fit_metrics\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.625 Preprocessor1_Model1\n2 roc_auc     binary         0.685 Preprocessor1_Model1\n3 brier_class binary         0.240 Preprocessor1_Model1\n\n\nFor the last part of fitting the model we utilize finalize_workflow; we add the model with workflow, the fitted logistic regression model and the highest roc auc.\nThen we evaluate the model using last_fit(), adding the final_lasso and hdata_split, our training data, and then we collect_metrics(). We fit the model to our training split and collect the output metrics\nAccuracy and roc_auc did the best with estimate values of 0.625 and 0.685, which is not terribly high.\nIn linear regression you can look at coefficients and conclude that things such as age correlate with the percentage chance to contract a certain disease. Because we use LASSO regression which works differently from regular regression, with its shrinkage towards zero, we can’t interpret the coefficients in the same way we would interpret regular linear regression.\n\nknitr::kable(\nfinal_lasso %&gt;%\n  fit(hdata_train) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip::vi(lambda = highest_roc_auc_lasso$penalty))\n\n\n\n\nVariable\nImportance\nSign\n\n\n\n\nnode_caps_yes\n0.5682117\nPOS\n\n\nirradiat_yes\n0.2840091\nPOS\n\n\nmenopause_premeno\n0.2513943\nPOS\n\n\ndeg_malig\n0.2077117\nPOS\n\n\ninv_nodes\n0.1532454\nPOS\n\n\ntumor_size\n0.0410034\nPOS\n\n\nage\n0.0000000\nNEG\n\n\nmenopause_lt40\n0.0000000\nNEG\n\n\nnode_caps_no\n0.0000000\nNEG\n\n\nbreast_right\n0.0000000\nNEG\n\n\nbreast_quad_central\n0.0000000\nNEG\n\n\nbreast_quad_left_low\n0.0000000\nNEG\n\n\nbreast_quad_left_up\n0.0000000\nNEG\n\n\nbreast_quad_right_low\n0.0000000\nNEG\n\n\nbreast_quad_right_up\n0.0000000\nNEG\n\n\n\n\n\nThis tells us what the most important variables are and their positive/negative impact. The table (fig) shows that presence of node caps is the most important factor that positively impacts the recurrence variable.\n\nfinal_lasso_fitted &lt;- fit(final_lasso, data = hdata_train)\npredicted_classes &lt;- predict(final_lasso_fitted, new_data = hdata_test)\npredicted_probs &lt;- predict(final_lasso_fitted, new_data = hdata_test, type = \"prob\")\nnames(predicted_probs) &lt;- c(\"pred_no_recur\", \"pred_recur\")\n\nresults &lt;- hdata_test %&gt;%\n  bind_cols(predicted_classes, predicted_probs)\n\nroc_auc &lt;- results %&gt;%\n  roc_auc(truth = recurrence, pred_no_recur) # Assuming pred_recur is the probability of recurrence = 1\n\nWe now test the training data on the test set and use the results to create some visualisation.\n\nresults %&gt;%\n  ggplot(aes(x = pred_recur, fill = as.factor(recurrence))) +\n  geom_histogram(binwidth = 0.03,\n                 position = \"identity\",\n                 alpha = 0.5) +\n  scale_fill_manual(values = c(\"no-recurrence-events\" = \"blue\", \"recurrence-events\" = \"red\"),\n                    labels = c(\"No recurrence\", \"recurrence\")) +\n  labs(title = \"Distribution of Predicted Probabilities\",\n       x = \"Predicted Probability of recurrence\",\n       y = \"Count\",\n       fill = \"Actual Outcome\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nGenerally the expected result of a good model would be a good split between the two outcomes and not a lot of overlap. In this case we have quite a bit of overlap, which is not entirely unexpected considering the somewhat lower roc value of 0.685. There is still room for improvement in the hypertuning of parameters.\n\nresults %&gt;%\n  roc_curve(truth = recurrence, pred_recur, event_level = \"second\") %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Corrected ROC Curve\", x = \"1 - Specificity\", y = \"Sensitivity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# roc_obj &lt;- roc(results$recurrence, results$pred_recur, levels = c(0, 1))\n# auc(roc_obj)  # Should return 0.9375\n# plot(roc_obj, col = \"blue\", lwd = 2)\n\nIn the ROC curve we can see that it does lean more towards the top end which increases the area under the curve, but it does stick towards the middle.\n\nconf_mat &lt;- results %&gt;%\n  conf_mat(truth = recurrence, estimate = .pred_class)\n\nautoplot(conf_mat, type = \"heatmap\") +\n  scale_fill_gradient(low = \"darkblue\", high = \"orange\") +\n  labs(title = \"Confusion Matrix Heatmap\", x = \"Predicted Class\", y = \"Actual Class\") +\n  theme_minimal()\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nIn the confusion matrix we can clearly see the results of the model on the test dataset. The predictions for the recurrence x recurrence and no-recurrence x no-recurrence is quite good, however 37% of the mistakes of recurrence events being wrongly predicted as no recurrence events, false negatives, is the least desired type of error that could happen in the context of the dataset.\nAccording to the dataset’s baseline model performance:   https://archive.ics.uci.edu/dataset/14/breast+cancer There are differing ranges of model performance . It might be that this machine learning model might be a bit weaker at predicting the recurrence with the provided data, random forest for example seems to be doing very well comparatively . However there are still optimizations that are able to be performed to potentially increase the Tidymodels’ performance .",
    "crumbs": [
      "Home",
      "Getting started with machine learning"
    ]
  },
  {
    "objectID": "007_R_Package.html",
    "href": "007_R_Package.html",
    "title": "R package",
    "section": "",
    "text": "R package",
    "crumbs": [
      "Home",
      "R package"
    ]
  },
  {
    "objectID": "001_CV.html",
    "href": "001_CV.html",
    "title": "CV",
    "section": "",
    "text": "CV\nAbout me:\nStudied life sciences with major in molecular biology, transitioning into the field of data science. Having experience in R programming and bash, I am eager to use my analytical skills on complex data sets combined with programming to gain data-driven insights into biological processes.\nPersonalia Naam   Alex Groot  Adres   Zuiderpoort 10  Postcode   2152 RG  Woonplaats   Nieuw-Vennep  Telefoonnummer:   0618336455  Mobiel nr.   0618336455  Email   Alex_groot@icloud.com  Geboortedatum   2 juli 1996 \nOpleiding  Biologie en Medisch Laboratoriumonderzoek HBO Bachelor  Specialisatie Biomolecular Research \nData science vaardigheden  R (programming language) Git Data visualisation Bash Microsoft Excel\nWerkervaring Project genes and proteins: Transfecteren van eukaryote cellen met recombinante plasmiden SDS-PAGE Western-blot\nProject sphingolipids: Literatuuronerzoek Werken met meerdere cellijnen (SKNAS, HEK293T, HEPG2). Opstellen en uitvoeren van assays (MTT, Amplex Red) Thin Layer Chromatography (TLC)",
    "crumbs": [
      "Home",
      "CV"
    ]
  },
  {
    "objectID": "002_Looking_ahead.html",
    "href": "002_Looking_ahead.html",
    "title": "Looking ahead",
    "section": "",
    "text": "Looking ahead\nTry, for yourself, to answer the following questions:\n\nWhere do I want to be in ~2 years time?\nHow am I doing now with respect to this goal?\nWhat would be the next skill to learn?\nMake a planning on how to start learning this new skill.\nIn 2 years time I will hopefully be working somewhere related to data sciences. As of now I am unsure of exactly where this will be and what skills will be required, therefore I am unable to answer exactly what I will need in order to achieve this goal. After my internship I won’t necessarily be looking for biology-related data science jobs, so I am also looking for skills that would be versatile in multiple fields of data science. One of these would be machine learning which is a skill I’m looking into but of which I’m still unsure as to how it is utilized.\nThe first thing to do would be to look at different companies and job openings to see what skills are required by them and then delve deeper into what these entail and how/what to apply these on.",
    "crumbs": [
      "Home",
      "Looking ahead"
    ]
  },
  {
    "objectID": "006_relational_databases.html",
    "href": "006_relational_databases.html",
    "title": "Relational databases",
    "section": "",
    "text": "Relational databases\n\n# https://lesmaterialen.rstudio.hu.nl/workflows-reader/relationaldb.html\n\n# https://robertochiosa.medium.com/import-export-r-packages-a6a122005e00\n\n# setwd(\"/Users/USER/Desktop/folder\")\n# packages &lt;- as.data.frame(installed.packages())  \n# write.csv(packages, 'packages.csv')\n\n# packages &lt;- read.csv(file.path('packages.csv'))[, -1]\n# base_packages &lt;- as.data.frame(installed.packages()) \n# to_install &lt;- setdiff(packages$Package, base_packages$Package) \n# install.packages(to_install)\n\n\nlibrary(here)\n\nhere() starts at /home/alexgroot/testing_qmd\n\nlibrary(dslabs)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ndengue_raw &lt;- read.csv2(here::here(\"data_raw/data_0060/dengue_data.csv\"),\n                        sep = \",\",\n                        dec = \".\",\n                        skip = 11)\nflu_raw &lt;- read.csv2(here::here(\"data_raw/data_0060/flu_data.csv\"),\n                     sep = \",\",\n                     skip = 11)\ngapminder_raw &lt;- gapminder\n\nobjective\nThe Gapminder dataset contains data regarding health and income for 184 countries from 1960 to 2016, such as: country, year, infant mortality per 1000, life expectancy in years, fertility in average number of children per woman, country population, GPD according to World Bankdev, continent and geographical region.\nThe Gapminder dataset is compared to data retrieved from Google trends. The two used datasets contain the Google trend data for the flu and dengue, which is correlated with the occurrence of said diseases in and around its observed trend data.\nThis exercise looks to compare the disease data to the general Gapminder data.\nThis data is then stored to a new PostgreSQL database, analysed and visualized.\n\n# inspecting the data made me annoyed with the fact that View is one of the few functions i use regularly and is capitalised.\nview &lt;- utils::View\n\n# View(dengue_raw) # works\n\n# view(dengue_raw) # does not work, on Mac this view opens in a separate xquartz window, opens in a seperate window in Windows.\n\nview &lt;- function(x, title = deparse(substitute(x))) {\n  if (exists(\"View\", envir = as.environment(\"tools:rstudio\"))) {\n    get(\"View\", envir = as.environment(\"tools:rstudio\"))(x, title)\n  } else {\n    utils::View(x, title)\n  }\n}\n\n# view(dengue_raw) # now checks for the rstudio env and uses it, if rstudio env does not exist default to regular View.\n\ninspecting data:\n\ndengue_raw %&gt;% head(n = c(2,4))\n\n        Date Argentina Bolivia Brazil\n1 2002-12-29        NA   0.101  0.073\n2 2003-01-05        NA   0.143  0.098\n\n\ndengue raw:    date: YYYY-MM-DD    country: value \n\nna.omit(flu_raw) %&gt;% head(n = c(2,4))\n\n          Date Argentina Australia Austria\n160 2006-01-15        91       136     920\n161 2006-01-22        83       121     840\n\n# na.omit is used to show what the data looks like\n\nflu raw:    date: YYYY-MM-DD    country: value \n\ngapminder %&gt;% head(n = 2)\n\n  country year infant_mortality life_expectancy fertility population\n1 Albania 1960            115.4           62.87      6.19    1636054\n2 Algeria 1960            148.2           47.50      7.65   11124892\n          gdp continent          region\n1          NA    Europe Southern Europe\n2 13828152297    Africa Northern Africa\n\n\ngapminder raw:    date: YYYY-//-//    country: country    life_expectancy: value    fertility: value    population: value    gdp: value \nThe best course of action would be to transform the flu and dengue data to fit into the Gapminder data. For this we need to change the date value into more general “year” values. We can choose between multiple strategies such as taking the highest value for a month, taking a median or an average. Next we need to rename the flu and dengue values with their own headers for easier identification when compared to the Gapminder data.\nWe will lose out some details regarding specific dates but as we’re comparing the data to the Gapminder “yearly” data we won’t be able to use these details.\n\n# date\n# country\n# activity\n\n# tidy data.\n# exclude the date to keep the column intact\ndengue_tidy &lt;-\n  dengue_raw %&gt;% pivot_longer(\n  cols = !`Date`,\n  names_to = \"country\",\n  values_to = \"activity\"\n)\n# change colname to lowercase\ncolnames(dengue_tidy)[1] &lt;- \"date\"\n\n# exclude the date to keep the column intact\nflu_tidy &lt;-\n  flu_raw %&gt;% pivot_longer(\n    cols = !`Date`,\n    names_to = \"country\",\n    values_to = \"activity\"\n  )\n\n# change colname to lowercase\ncolnames(flu_tidy)[1] &lt;- \"date\"\n\n# gapminder is loaded as is\ngapminder_tidy &lt;-\n  gapminder_raw\n\n\n# add year column for Gapminder dataset\nflu_tidy$year &lt;- \n  as.Date(flu_tidy$date) %&gt;% format(\"%Y\")\n\ndengue_tidy$year &lt;-\n  as.Date(dengue_tidy$date) %&gt;% format(\"%Y\")\n\n# replace missing NA values with 0\nflu_tidy$activity[is.na(flu_tidy$activity)] &lt;- 0\ndengue_tidy$activity[is.na(dengue_tidy$activity)] &lt;- 0\n\n# group dataframe by country and year, add mean activity column for parity with Gapminder dataset\n\nflu_tidy_mean &lt;-\n  flu_tidy %&gt;% \n  group_by(`country`, `year`) %&gt;% \n  mutate(flu_activity_mean = mean(activity))\n\ndengue_tidy_mean &lt;-\n  dengue_tidy %&gt;% \n  group_by(`country`, `year`) %&gt;% \n  mutate(dengue_tidy_mean = mean(activity))\n\n\n# Save files as csv and rds\nwrite.csv(flu_tidy_mean, here(\"data_output/data_0060/flu.csv\"))\nsaveRDS(flu_tidy_mean, here(\"data_output/data_0060/flu.rds\"))\n\nwrite.csv(dengue_tidy_mean, here(\"data_output/data_0060/dengue.csv\"))\nsaveRDS(dengue_tidy_mean, here(\"data_output/data_0060/dengue.rds\"))\n\nwrite.csv(gapminder_tidy, here(\"data_output/data_0060/gapminder.csv\"))\nsaveRDS(gapminder_tidy, here(\"data_output/data_0060/gapminder.rds\"))\n\n# export to SQL\n# db_write_table(con, \"gapminder_table\", gapminder_tidy)",
    "crumbs": [
      "Home",
      "Relational databases"
    ]
  },
  {
    "objectID": "004_C_elegans.html",
    "href": "004_C_elegans.html",
    "title": "C elegans",
    "section": "",
    "text": "C elegans\n\nlibrary(here)\n\nhere() starts at /home/alexgroot/testing_qmd\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(drc)\n\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n'drc' has been loaded.\n\nPlease cite R and 'drc' if used for a publication,\nfor references type 'citation()' and 'citation('drc')'.\n\n\nAttaching package: 'drc'\n\nThe following objects are masked from 'package:stats':\n\n    gaussian, getInitial\n\ntheme_set(theme_bw())\n\n\ndata_0040 &lt;-\n  readxl::read_excel(here(\"data_raw/data_0040/CE.LIQ.FLOW.062_Tidydata.xlsx\"))\ndata_0040\n\n# A tibble: 360 × 34\n   plateRow plateColumn vialNr dropCode expType    expReplicate expName        \n   &lt;lgl&gt;    &lt;lgl&gt;        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          \n 1 NA       NA               1 a        experiment            3 CE.LIQ.FLOW.062\n 2 NA       NA               1 b        experiment            3 CE.LIQ.FLOW.062\n 3 NA       NA               1 c        experiment            3 CE.LIQ.FLOW.062\n 4 NA       NA               1 d        experiment            3 CE.LIQ.FLOW.062\n 5 NA       NA               1 e        experiment            3 CE.LIQ.FLOW.062\n 6 NA       NA               2 a        experiment            3 CE.LIQ.FLOW.062\n 7 NA       NA               2 b        experiment            3 CE.LIQ.FLOW.062\n 8 NA       NA               2 c        experiment            3 CE.LIQ.FLOW.062\n 9 NA       NA               2 d        experiment            3 CE.LIQ.FLOW.062\n10 NA       NA               2 e        experiment            3 CE.LIQ.FLOW.062\n# ℹ 350 more rows\n# ℹ 27 more variables: expDate &lt;dttm&gt;, expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;,\n#   expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;dbl&gt;, compCASRN &lt;chr&gt;,\n#   compName &lt;chr&gt;, compConcentration &lt;chr&gt;, compUnit &lt;chr&gt;,\n#   compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;,\n#   elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;,\n#   bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, …\n\n\nColumn RawData has missing data in cells 192-196, or samples in vialNr 3, compName napthalene. These data points are NA in the data when loaded into R studio. Most compUnits use nM, Ethanol and S-medium use pct.\n\n# compConcentration doesn't show up on a graph properly because its values are of chr type\n# change type compConcentration to numeric\ndata_0040_tidy &lt;- data_0040\ndata_0040_tidy$compConcentration &lt;- \n  as.numeric(data_0040$compConcentration)\n\nWarning: NAs introduced by coercion\n\n\n\n# create ggplot, adding concentration to x-axis and measured data to y-axis\n# control/experiment are defined by shape, components are defined by colour\ndata_0040_graph &lt;- \n  as_tibble(data_0040_tidy) %&gt;%\n  ggplot(aes(x = compConcentration,\n             y = RawData,\n             shape = expType,\n             colour = compName)) +\n  geom_point(position = position_jitter(w = 0.03, h = 0)) + # jiter is added for viewing benefits\n  scale_x_continuous(trans = log10_trans()) + # log10 scale is applied to make results more readable.\n  labs(title = \"title\",\n       subtitle = \"subtitle\")\nprint(data_0040_graph)\n\nWarning in scale_x_continuous(trans = log10_trans()): log-10 transformation\nintroduced infinite values.\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThe positive control = Ethanol\nThe negative control = S-medium\n\n\n# facet wrap to show individual graphs\ndata_0040_graph +\n  facet_wrap(vars(compName))\n\nWarning in scale_x_continuous(trans = log10_trans()): log-10 transformation\nintroduced infinite values.\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nWith the components split it is easier to see that there seems to be a downward trend in the offspring count once the concentration of the components increase.\n\n\n# data is normalized for negative control by setting the mean of negative control to 1\n\nnegative_control_mean &lt;- data_0040_tidy %&gt;%\n  filter(expType == \"controlNegative\") %&gt;%\n  summarise(mean_value = mean(RawData, na.rm = TRUE)) %&gt;%\n  pull(mean_value) # get the value from the mean_value in the tibble without having to use $mean_value\n\n# normalize the data:\n# if the data is negative control, set value to 1\n# else divide raw data by negative control mean to create value as a fraction\ndata_0040_temp &lt;- data_0040_tidy\ndata_0040_normalized &lt;- data_0040_temp %&gt;%\n  mutate(\n    NormalizedData = if_else(expType == \"controlNegative\", 1, RawData / negative_control_mean)\n  )\n\n# print the normalized dataset\nprint(data_0040_normalized)\n\n# A tibble: 360 × 35\n   plateRow plateColumn vialNr dropCode expType    expReplicate expName        \n   &lt;lgl&gt;    &lt;lgl&gt;        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          \n 1 NA       NA               1 a        experiment            3 CE.LIQ.FLOW.062\n 2 NA       NA               1 b        experiment            3 CE.LIQ.FLOW.062\n 3 NA       NA               1 c        experiment            3 CE.LIQ.FLOW.062\n 4 NA       NA               1 d        experiment            3 CE.LIQ.FLOW.062\n 5 NA       NA               1 e        experiment            3 CE.LIQ.FLOW.062\n 6 NA       NA               2 a        experiment            3 CE.LIQ.FLOW.062\n 7 NA       NA               2 b        experiment            3 CE.LIQ.FLOW.062\n 8 NA       NA               2 c        experiment            3 CE.LIQ.FLOW.062\n 9 NA       NA               2 d        experiment            3 CE.LIQ.FLOW.062\n10 NA       NA               2 e        experiment            3 CE.LIQ.FLOW.062\n# ℹ 350 more rows\n# ℹ 28 more variables: expDate &lt;dttm&gt;, expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;,\n#   expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;dbl&gt;, compCASRN &lt;chr&gt;,\n#   compName &lt;chr&gt;, compConcentration &lt;dbl&gt;, compUnit &lt;chr&gt;,\n#   compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;,\n#   elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;,\n#   bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, …\n\n# print normalized dataset as ggplot\ndata_0040_normalized_graph &lt;-\n  data_0040_normalized %&gt;%\n  ggplot(aes(x = compConcentration,\n             y = NormalizedData,\n             shape = expType,\n             colour = compName)) +\n  geom_point(position = position_jitter(w = 0.03, h = 0)) +\n  scale_x_continuous(trans = log10_trans(),\n                     labels = label_log()) +\n  labs(title = \"title\",\n       subtitle = \"subtitle\")\nprint(data_0040_normalized_graph)\n\nWarning in scale_x_continuous(trans = log10_trans(), labels = label_log()):\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\ndata_0040_normalized_graph &lt;-\n  data_0040_normalized %&gt;%\n  ggplot(aes(x = compConcentration,\n             y = NormalizedData,\n             shape = expType,\n             colour = compName)) +\n  geom_point(position = position_jitter(w = 0.03, h = 0)) +\n  scale_x_log10() +\n  labs(title = \"test\",\n       subtitle = \"subtitle\")\nprint(data_0040_normalized_graph)\n\nWarning in scale_x_log10(): log-10 transformation introduced infinite values.\nRemoved 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nfinish dose response with drc here\n\n# \ndata_0040_tidy %&gt;% view",
    "crumbs": [
      "Home",
      "C elegans"
    ]
  },
  {
    "objectID": "009_Parameterized_report.html",
    "href": "009_Parameterized_report.html",
    "title": "Parameterized report",
    "section": "",
    "text": "Parameterized report",
    "crumbs": [
      "Home",
      "Parameterized report"
    ]
  }
]